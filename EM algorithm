import numpy as np
import matplotlib.pyplot as plt

def gaussian_pdf(x, mean, std):
    """1D Gaussian probability density."""
    coef = 1.0 / (np.sqrt(2 * np.pi) * std)
    exponent = -0.5 * ((x - mean) / std) ** 2
    return coef * np.exp(exponent)

# 1) Generate synthetic 1D data from two Gaussians
np.random.seed(42)
n_samples = 300
data = np.concatenate([
    np.random.normal(loc=0.0, scale=1.0, size=n_samples // 2),
    np.random.normal(loc=5.0, scale=1.5, size=n_samples // 2),
])
data = data.reshape(-1, 1)   # shape (n,1)
n = data.shape[0]

# 2) Initialize parameters for k components
k = 2
# Means: choose k random points from data
mu = np.random.choice(data.flatten(), k)
# Stds: initialize to 1
sigma = np.ones(k)
# Mixing coefficients: uniform
pi = np.ones(k) / k

log_likelihoods = []

# 3) EM loop
for it in range(100):
    # --- E-step: compute responsibilities (n x k)
    resp = np.zeros((n, k))
    for j in range(k):
        resp[:, j] = pi[j] * gaussian_pdf(data.flatten(), mu[j], sigma[j])
    resp_sum = resp.sum(axis=1, keepdims=True)
    resp = resp / resp_sum  # normalize rows

    # --- M-step: update parameters
    N_k = resp.sum(axis=0)  # shape (k,)
    for j in range(k):
        # update mean
        mu[j] = (resp[:, j] @ data.flatten()) / N_k[j]
        # update std
        diff = data.flatten() - mu[j]
        sigma[j] = np.sqrt((resp[:, j] * (diff ** 2)).sum() / N_k[j])
        # update mixing coeff
        pi[j] = N_k[j] / n

    # Compute log-likelihood
    weighted_pdfs = np.zeros(n)
    for j in range(k):
        weighted_pdfs += pi[j] * gaussian_pdf(data.flatten(), mu[j], sigma[j])
    ll = np.sum(np.log(weighted_pdfs))
    log_likelihoods.append(ll)

    # Convergence check
    if it > 0 and abs(log_likelihoods[-1] - log_likelihoods[-2]) < 1e-6:
        print(f"Converged at iteration {it}")
        break

# 4) Print results
print("Estimated means:       ", mu)
print("Estimated std devs:    ", sigma)
print("Estimated mixing coeffs:", pi)

# 5) Plot fitted GMM vs. data histogram
x_axis = np.linspace(data.min() - 1, data.max() + 1, 1000)
pdf = np.zeros_like(x_axis)
for j in range(k):
    pdf += pi[j] * gaussian_pdf(x_axis, mu[j], sigma[j])

plt.figure(figsize=(8, 4))
plt.hist(data, bins=30, density=True, alpha=0.5, label="Data histogram")
plt.plot(x_axis, pdf, 'r-', lw=2, label="EM GMM density")
plt.title("EM-estimated Gaussian Mixture")
plt.xlabel("x")
plt.ylabel("Density")
plt.legend()
plt.grid(True)
plt.show()

# 6) Plot log-likelihood over iterations
plt.figure(figsize=(6, 3))
plt.plot(log_likelihoods, marker='o')
plt.title("Log-Likelihood over Iterations")
plt.xlabel("Iteration")
plt.ylabel("Log-Likelihood")
plt.grid(True)
plt.show()
